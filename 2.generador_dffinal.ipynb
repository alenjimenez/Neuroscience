{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATOS CIENTÍFICOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ITBA - Maestría en Ciencia de Datos - 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trabajo Práctico - Alen Jiménez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El objetivo de esta notebook es clasificar datos provenientes de sensores de ondas cerebrales, que se estructuran en series de tiempo.\n",
    "- Las ondas cerebrales que van a ser estudiadas se corresponden con dos comportamientos: \"ojos abiertos\" y \"pestañeos\".\n",
    "- Se va a intentar construir un modelo de clasificación que pueda distinguir estos dos comportamientos a partir de los datos provistos por las ondas cerebrales.\n",
    "- El análisis descriptivo de todas las series disponibles se encuentra en la notebook analisis_filtros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de Contenidos\n",
    "* [Set Up](#setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up <a class = 'anchor' id = 'setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os\n",
    "import sys, select\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import firwin, remez, kaiser_atten, kaiser_beta\n",
    "from scipy.signal import butter, filtfilt, buttord\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import find_peaks\n",
    "from collections import Counter\n",
    "#from xgboost import XGBClassifier # Clasificador de XGBoost\n",
    "#from bayes_opt import BayesianOptimization # Optimización Bayesiana\n",
    "from sklearn.model_selection import cross_val_score, train_test_split # Cross Validation y partición del dataset en Train y Test\n",
    "from sklearn.metrics import accuracy_score # Para medición del accuracy una vez que hagamos el XGBoost con los mejores parametros que nos da la optimización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio actual de trabajo: C:\\itba_datos_geograficos\\ramele\\tp\n"
     ]
    }
   ],
   "source": [
    "# Directorio de trabajo\n",
    "\n",
    "directorio_de_trabajo = 'C:/itba_datos_geograficos/ramele/tp'\n",
    "os.chdir(directorio_de_trabajo)\n",
    "print(f'Directorio actual de trabajo: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los archivos\n",
    "\n",
    "archivo1 = 'ojosabiertos'\n",
    "archivo2 = 'pestaneos'\n",
    "\n",
    "df1 = pd.read_csv('datos/%s.dat' % archivo1\n",
    "                  , delimiter = ' '\n",
    "                  , names = ['timestamp','counter','eeg','attention','meditation','blinking'])\n",
    "\n",
    "df2 = pd.read_csv('datos/%s.dat' % archivo2\n",
    "                  , delimiter = ' '\n",
    "                  , names = ['timestamp','counter','eeg','attention','meditation','blinking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30850, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30840, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medida: Densidad Espectral\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def psd(y):\n",
    "    # Number of samplepoints\n",
    "    N = 512\n",
    "    # sample spacing\n",
    "    T = 1.0 / 512.0\n",
    "    # From 0 to N, N*T, 2 points.\n",
    "    #x = np.linspace(0.0, 1.0, N)\n",
    "    #y = 1*np.sin(10.0 * 2.0*np.pi*x) + 9*np.sin(20.0 * 2.0*np.pi*x)\n",
    "\n",
    "\n",
    "    # Original Bandpass\n",
    "    fs = 512.0\n",
    "    fso2 = fs/2\n",
    "    #Nd,wn = buttord(wp=[9/fso2,11/fso2], ws=[8/fso2,12/fso2],\n",
    "    #   gpass=3.0, gstop=40.0)\n",
    "    #b,a = butter(Nd,wn,'band')\n",
    "    #y = filtfilt(b,a,y)\n",
    "\n",
    "    y = butter_bandpass_filter(y, 128.0, 135.0, fs, order=6)\n",
    "\n",
    "\n",
    "    yf = fft(y)\n",
    "    #xf = np.linspace(0.0, int(1.0/(2.0*T)), int(N/2))\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.plot(xf, 2.0/N * np.abs(yf[0:int(N/2)]))\n",
    "    #plt.axis((0,60,0,1))\n",
    "    #plt.grid()\n",
    "    #plt.show()\n",
    "\n",
    "    return np.sum(np.abs(yf[0:int(N/2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medida: Crest Factor\n",
    "\n",
    "def crest_factor(x):\n",
    "    return np.max(np.abs(x))/np.sqrt(np.mean(np.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medidas: activity, morbidity, complexity\n",
    "\n",
    "def hjorth(a):\n",
    "    r\"\"\"\n",
    "    Compute Hjorth parameters [HJO70]_.\n",
    "    .. math::\n",
    "        Activity = m_0 = \\sigma_{a}^2\n",
    "    .. math::\n",
    "        Complexity = m_2 = \\sigma_{d}/ \\sigma_{a}\n",
    "    .. math::\n",
    "        Morbidity = m_4 =  \\frac{\\sigma_{dd}/ \\sigma_{d}}{m_2}\n",
    "    Where:\n",
    "    :math:`\\sigma_{x}^2` is the mean power of a signal :math:`x`. That is, its variance, if it's mean is zero.\n",
    "    :math:`a`, :math:`d` and :math:`dd` represent the original signal, its first and second derivatives, respectively.\n",
    "    .. note::\n",
    "        **Difference with PyEEG:**\n",
    "        Results is different from [PYEEG]_ which appear to uses a non normalised (by the length of the signal) definition of the activity:\n",
    "        .. math::\n",
    "            \\sigma_{a}^2 = \\sum{\\mathbf{x}[i]^2}\n",
    "        As opposed to\n",
    "        .. math::\n",
    "            \\sigma_{a}^2 = \\frac{1}{n}\\sum{\\mathbf{x}[i]^2}\n",
    "    :param a: a one dimensional floating-point array representing a time series.\n",
    "    :type a: :class:`~numpy.ndarray` or :class:`~pyrem.time_series.Signal`\n",
    "    :return: activity, complexity and morbidity\n",
    "    :rtype: tuple(float, float, float)\n",
    "    Example:\n",
    "    >>> import pyrem as pr\n",
    "    >>> import numpy as np\n",
    "    >>> # generate white noise:\n",
    "    >>> noise = np.random.normal(size=int(1e4))\n",
    "    >>> activity, complexity, morbidity = pr.univariate.hjorth(noise)\n",
    "    \"\"\"\n",
    "\n",
    "    first_deriv = np.diff(a)\n",
    "    second_deriv = np.diff(a,2)\n",
    "\n",
    "    var_zero = np.mean(a ** 2)\n",
    "    var_d1 = np.mean(first_deriv ** 2)\n",
    "    var_d2 = np.mean(second_deriv ** 2)\n",
    "\n",
    "    activity = var_zero\n",
    "    morbidity = np.sqrt(var_d1 / var_zero)\n",
    "    complexity = np.sqrt(var_d2 / var_d1) / morbidity\n",
    "\n",
    "    return activity, morbidity, complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medida: Petrosian Fractal Dimension\n",
    "\n",
    "def pfd(a):\n",
    "    r\"\"\"\n",
    "    Compute Petrosian Fractal Dimension of a time series [PET95]_.\n",
    "    It is defined by:\n",
    "    .. math::\n",
    "        \\frac{log(N)}{log(N) + log(\\frac{N}{N+0.4N_{\\delta}})}\n",
    "    .. note::\n",
    "        **Difference with PyEEG:**\n",
    "        Results is different from [PYEEG]_ which implemented an apparently erroneous formulae:\n",
    "        .. math::\n",
    "            \\frac{log(N)}{log(N) + log(\\frac{N}{N}+0.4N_{\\delta})}\n",
    "    Where:\n",
    "    :math:`N` is the length of the time series, and\n",
    "    :math:`N_{\\delta}` is the number of sign changes.\n",
    "    :param a: a one dimensional floating-point array representing a time series.\n",
    "    :type a: :class:`~numpy.ndarray` or :class:`~pyrem.time_series.Signal`\n",
    "    :return: the Petrosian Fractal Dimension; a scalar.\n",
    "    :rtype: float\n",
    "    Example:\n",
    "    >>> import pyrem as pr\n",
    "    >>> import numpy as np\n",
    "    >>> # generate white noise:\n",
    "    >>> noise = np.random.normal(size=int(1e4))\n",
    "    >>> pr.univariate.pdf(noise)\n",
    "    \"\"\"\n",
    "\n",
    "    diff = np.diff(a)\n",
    "    # x[i] * x[i-1] for i in t0 -> tmax\n",
    "    prod = diff[1:-1] * diff[0:-2]\n",
    "\n",
    "    # Number of sign changes in derivative of the signal\n",
    "    N_delta = np.sum(prod < 0)\n",
    "    n = len(a)\n",
    "\n",
    "    return np.log(n)/(np.log(n)+np.log(n/(n+0.4*N_delta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for dffinal\n",
    "columns = ['target', 'ptp', 'rms', 'cf'\n",
    "           , 'entropy', 'activity', 'complexity'\n",
    "           , 'morbidity', 'fractal', 'psd9']\n",
    "\n",
    "dffinal = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Define the window size\n",
    "window_size = 512\n",
    "\n",
    "# Function to compute measures for a given window\n",
    "def compute_measures(data):\n",
    "    # Add your logic here to compute measure1 to measure9\n",
    "    # For example, assuming 'data' is a pandas Series for the 'eeg' column:\n",
    "    ptp = abs(np.max(data)) + abs(np.min(data))\n",
    "    rms = np.sqrt(np.mean(data**2))\n",
    "    cf = crest_factor(data)\n",
    "    entropy = stats.entropy(list(Counter(data).values()), base=2)\n",
    "    activity, complexity, morbidity = hjorth(data)\n",
    "    fractal = pfd(data)\n",
    "    psd(data)\n",
    "    # Add more measures as needed\n",
    "    return ptp,rms,cf,entropy,activity, complexity, morbidity,fractal,psd # Add other measures as needed\n",
    "\n",
    "# Populate dffinal using data from df1\n",
    "for i in range(len(df1) - window_size + 1):\n",
    "    window_data = df1['eeg'].iloc[i : i + window_size]\n",
    "    measures = compute_measures(window_data)\n",
    "    dffinal = dffinal.append({'target': 0\n",
    "                              , 'ptp': measures[0]\n",
    "                              , 'rms': measures[1]\n",
    "                              , 'cf': measures[2]\n",
    "                              , 'entropy': measures[3]\n",
    "                              , 'activity': measures[4]\n",
    "                              , 'complexity': measures[5]\n",
    "                              , 'morbidity': measures[6]\n",
    "                              , 'fractal': measures[7]\n",
    "                              , 'psd': measures[8]\n",
    "                              }\n",
    "                             , ignore_index=True)\n",
    "\n",
    "# Populate dffinal using data from df2\n",
    "for i in range(len(df2) - window_size + 1):\n",
    "    window_data = df2['eeg'].iloc[i : i + window_size]\n",
    "    measures = compute_measures(window_data)\n",
    "    dffinal = dffinal.append({'target': 1\n",
    "                              , 'ptp': measures[0]\n",
    "                              , 'rms': measures[1]\n",
    "                              , 'cf': measures[2]\n",
    "                              , 'entropy': measures[3]\n",
    "                              , 'activity': measures[4]\n",
    "                              , 'complexity': measures[5]\n",
    "                              , 'morbidity': measures[6]\n",
    "                              , 'fractal': measures[7]\n",
    "                              , 'psd': measures[8]\n",
    "                              }\n",
    "                             , ignore_index=True)\n",
    "\n",
    "# Reset index of the final dataframe\n",
    "dffinal.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60668, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ptp</th>\n",
       "      <th>rms</th>\n",
       "      <th>cf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>activity</th>\n",
       "      <th>complexity</th>\n",
       "      <th>morbidity</th>\n",
       "      <th>fractal</th>\n",
       "      <th>psd9</th>\n",
       "      <th>psd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1055</td>\n",
       "      <td>145.506732</td>\n",
       "      <td>4.219736</td>\n",
       "      <td>7.352986</td>\n",
       "      <td>21172.208984</td>\n",
       "      <td>0.188514</td>\n",
       "      <td>5.499801</td>\n",
       "      <td>1.020167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;function psd at 0x000001F30118E1E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1072</td>\n",
       "      <td>146.862007</td>\n",
       "      <td>4.180795</td>\n",
       "      <td>7.359324</td>\n",
       "      <td>21568.449219</td>\n",
       "      <td>0.186804</td>\n",
       "      <td>5.548790</td>\n",
       "      <td>1.020052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;function psd at 0x000001F30118E1E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1106</td>\n",
       "      <td>148.430658</td>\n",
       "      <td>4.136612</td>\n",
       "      <td>7.367413</td>\n",
       "      <td>22031.660156</td>\n",
       "      <td>0.185107</td>\n",
       "      <td>5.592249</td>\n",
       "      <td>1.019937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;function psd at 0x000001F30118E1E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1138</td>\n",
       "      <td>150.193528</td>\n",
       "      <td>4.088059</td>\n",
       "      <td>7.367413</td>\n",
       "      <td>22558.095703</td>\n",
       "      <td>0.183148</td>\n",
       "      <td>5.644323</td>\n",
       "      <td>1.019937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;function psd at 0x000001F30118E1E0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1159</td>\n",
       "      <td>152.089419</td>\n",
       "      <td>4.037099</td>\n",
       "      <td>7.372794</td>\n",
       "      <td>23131.191406</td>\n",
       "      <td>0.180834</td>\n",
       "      <td>5.718295</td>\n",
       "      <td>1.019937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;function psd at 0x000001F30118E1E0&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target   ptp         rms        cf   entropy      activity  complexity  \\\n",
       "0      0  1055  145.506732  4.219736  7.352986  21172.208984    0.188514   \n",
       "1      0  1072  146.862007  4.180795  7.359324  21568.449219    0.186804   \n",
       "2      0  1106  148.430658  4.136612  7.367413  22031.660156    0.185107   \n",
       "3      0  1138  150.193528  4.088059  7.367413  22558.095703    0.183148   \n",
       "4      0  1159  152.089419  4.037099  7.372794  23131.191406    0.180834   \n",
       "\n",
       "   morbidity   fractal  psd9                                   psd  \n",
       "0   5.499801  1.020167   NaN  <function psd at 0x000001F30118E1E0>  \n",
       "1   5.548790  1.020052   NaN  <function psd at 0x000001F30118E1E0>  \n",
       "2   5.592249  1.019937   NaN  <function psd at 0x000001F30118E1E0>  \n",
       "3   5.644323  1.019937   NaN  <function psd at 0x000001F30118E1E0>  \n",
       "4   5.718295  1.019937   NaN  <function psd at 0x000001F30118E1E0>  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rms</th>\n",
       "      <th>cf</th>\n",
       "      <th>entropy</th>\n",
       "      <th>activity</th>\n",
       "      <th>complexity</th>\n",
       "      <th>morbidity</th>\n",
       "      <th>fractal</th>\n",
       "      <th>psd9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>60668.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202.681089</td>\n",
       "      <td>3.301256</td>\n",
       "      <td>7.804700</td>\n",
       "      <td>44471.120171</td>\n",
       "      <td>0.122871</td>\n",
       "      <td>8.655943</td>\n",
       "      <td>1.018172</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>58.237037</td>\n",
       "      <td>0.556903</td>\n",
       "      <td>0.411772</td>\n",
       "      <td>20337.960422</td>\n",
       "      <td>0.066470</td>\n",
       "      <td>2.007113</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>34.916949</td>\n",
       "      <td>1.982388</td>\n",
       "      <td>6.366749</td>\n",
       "      <td>1219.193359</td>\n",
       "      <td>0.058728</td>\n",
       "      <td>1.960549</td>\n",
       "      <td>1.012918</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>188.134446</td>\n",
       "      <td>2.943870</td>\n",
       "      <td>7.635770</td>\n",
       "      <td>35394.569824</td>\n",
       "      <td>0.087742</td>\n",
       "      <td>7.960362</td>\n",
       "      <td>1.017153</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>212.062518</td>\n",
       "      <td>3.273976</td>\n",
       "      <td>7.912101</td>\n",
       "      <td>44970.511719</td>\n",
       "      <td>0.106085</td>\n",
       "      <td>8.991340</td>\n",
       "      <td>1.018317</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>237.413896</td>\n",
       "      <td>3.596657</td>\n",
       "      <td>8.094257</td>\n",
       "      <td>56365.357910</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>9.968221</td>\n",
       "      <td>1.019360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>345.913482</td>\n",
       "      <td>8.218705</td>\n",
       "      <td>8.448871</td>\n",
       "      <td>119656.136719</td>\n",
       "      <td>0.555771</td>\n",
       "      <td>13.552365</td>\n",
       "      <td>1.021890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                rms            cf       entropy       activity    complexity  \\\n",
       "count  60668.000000  60668.000000  60668.000000   60668.000000  60668.000000   \n",
       "mean     202.681089      3.301256      7.804700   44471.120171      0.122871   \n",
       "std       58.237037      0.556903      0.411772   20337.960422      0.066470   \n",
       "min       34.916949      1.982388      6.366749    1219.193359      0.058728   \n",
       "25%      188.134446      2.943870      7.635770   35394.569824      0.087742   \n",
       "50%      212.062518      3.273976      7.912101   44970.511719      0.106085   \n",
       "75%      237.413896      3.596657      8.094257   56365.357910      0.125235   \n",
       "max      345.913482      8.218705      8.448871  119656.136719      0.555771   \n",
       "\n",
       "          morbidity       fractal  psd9  \n",
       "count  60668.000000  60668.000000   0.0  \n",
       "mean       8.655943      1.018172   NaN  \n",
       "std        2.007113      0.001531   NaN  \n",
       "min        1.960549      1.012918   NaN  \n",
       "25%        7.960362      1.017153   NaN  \n",
       "50%        8.991340      1.018317   NaN  \n",
       "75%        9.968221      1.019360   NaN  \n",
       "max       13.552365      1.021890   NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61690"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape[0]+df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30850"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30339\n",
       "1    30329\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffinal.to_csv('datos/dffinal.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
